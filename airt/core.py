# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/core.ipynb.

# %% auto 0
__all__ = ['HF_SD_MODEL', 'HF_CLIP_MODEL', 'VAE_ENCODE_SCALE', 'VAE_DECODE_SCALE', 'lms_scheduler', 'euler_scheduler',
           'euler_a_scheduler', 'pndm_scheduler', 'pndms_scheduler', 'SCHEDULERS', 'pipe', 'vae', 'tokenizer',
           'text_encoder', 'unet', 'scheduler', 'generator', 'pil_to_latents', 'latents_to_pils', 'generate_image_grid',
           'get_image_size_from_aspect_ratio', 'pil_to_b64', 'b64_to_pil', 'file_to_b64', 'b64_to_file',
           'convert_gif_to_mp4', 'latents_to_animation', 'Config', 'AIrtRequest', 'AIrtResponse',
           'get_pipe_params_from_airt_req', 'text2image', 'image2image', 'handle_airt_request']

# %% ../nbs/core.ipynb 4
import torch
from torch import autocast
from torchvision import transforms as tfms

from transformers import CLIPTextModel, CLIPTokenizer
from transformers import logging

from diffusers.models import AutoencoderKL, UNet2DConditionModel
from diffusers.schedulers import (
    EulerDiscreteScheduler,
    EulerAncestralDiscreteScheduler, 
    LMSDiscreteScheduler,
    DPMSolverMultistepScheduler,
    PNDMScheduler,
)

from diffusers import AltDiffusionPipeline

from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import (
    StableDiffusionPipelineOutput, 
    StableDiffusionPipeline,
)
# from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img_v0 import StableDiffusionImg2ImgPipeline

import os
import io
import inspect
import requests
import random
from tqdm.auto import tqdm
import PIL
from matplotlib import pyplot as plt
import numpy as np
import base64
from typing import List, Union, Tuple, Dict, Any
from pprint import pprint
import tempfile

# serving
import pydantic

logging.set_verbosity_error()

if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    # https://huggingface.co/docs/diffusers/optimization/mps
    device = "mps"
else:
    device = "cpu"

# %% ../nbs/core.ipynb 6
# HF_SD_MODEL = "BAAI/AltDiffusion-m9"
# HF_SD_MODEL = "runwayml/stable-diffusion-v1-5"
# HF_SD_MODEL = "stabilityai/stable-diffusion-2"
HF_SD_MODEL = "leemeng/anime-65000"

HF_CLIP_MODEL = "openai/clip-vit-large-patch14"

VAE_ENCODE_SCALE = 0.18215
VAE_DECODE_SCALE = 1 / VAE_ENCODE_SCALE

# %% ../nbs/core.ipynb 9
lms_scheduler = LMSDiscreteScheduler.from_pretrained(HF_SD_MODEL, subfolder="scheduler")
euler_scheduler = EulerDiscreteScheduler.from_pretrained(HF_SD_MODEL, subfolder="scheduler")
euler_a_scheduler = EulerAncestralDiscreteScheduler.from_pretrained(HF_SD_MODEL, subfolder="scheduler")
pndm_scheduler = DPMSolverMultistepScheduler.from_pretrained(HF_SD_MODEL, subfolder="scheduler")
pndms_scheduler = PNDMScheduler.from_pretrained(HF_SD_MODEL, subfolder="scheduler")


# %% ../nbs/core.ipynb 10
SCHEDULERS = {
    "euler": euler_scheduler,
    "euler_a": euler_a_scheduler,
    "lms": lms_scheduler,
    "pndm": pndm_scheduler,
    "pndms": pndms_scheduler,
}

# %% ../nbs/core.ipynb 11
if HF_SD_MODEL == "BAAI/AltDiffusion-m9":
    DEFAULT_PIPE_CLS = AltDiffusionPipeline
    DEFAULT_SCHEDULER = pndm_scheduler
    DEFAULT_REVISION = None
else:
    DEFAULT_PIPE_CLS = StableDiffusionPipeline
    DEFAULT_SCHEDULER = euler_scheduler
    DEFAULT_REVISION = "fp16"

# %% ../nbs/core.ipynb 13
if device == "mps":
    pipe = DEFAULT_PIPE_CLS.from_pretrained(
        HF_SD_MODEL, 
        scheduler=DEFAULT_SCHEDULER,
        requires_safety_checker=False
    )
else:
    pipe = DEFAULT_PIPE_CLS.from_pretrained(
        HF_SD_MODEL, 
        torch_dtype=torch.float16, 
#         revision=DEFAULT_REVISION,
        scheduler=DEFAULT_SCHEDULER,
        requires_safety_checker=False
    )

# pipe.safety_checker = None
pipe = pipe.to(device)
# pipe.enable_xformers_memory_efficient_attention()
# pipe.enable_attention_slicing()

# %% ../nbs/core.ipynb 15
vae = pipe.vae
tokenizer = pipe.tokenizer
text_encoder = pipe.text_encoder
unet = pipe.unet
scheduler = pipe.scheduler
generator = torch.Generator()

# %% ../nbs/core.ipynb 20
# i2i_pipe = StableDiffusionImg2ImgPipeline(**pipe.components)

# %% ../nbs/core.ipynb 27
@torch.no_grad()
def pil_to_latents(im: PIL.Image.Image) -> torch.Tensor:
    """
    Transform single image into single latent in a batch w/ shape = (1, 4, 64, 64)
    """
    device = vae.device.type
    tensor = tfms.ToTensor()(im).unsqueeze(0).to(device)
    tensor = tensor * 2 - 1
    with torch.autocast(device):
        latent = vae.encode(tensor) 
    torch.cuda.empty_cache()
    
    return VAE_ENCODE_SCALE * latent.latent_dist.sample()

# %% ../nbs/core.ipynb 30
@torch.no_grad()
def latents_to_pils(
    latents: Union[torch.Tensor, List[torch.Tensor]], 
    batch_size=4
) -> List[PIL.Image.Image]:
    """
    Transform batch of latent back to list of pil images
    - latents: shape(#latents, channels, heights, width)
    """
    
    device = vae.device.type
    if type(latents) is list:
        latents = torch.concat(latents)
        
    shape = latents.shape
    assert len(shape) == 4  # (b, c, h, w)
    
    n = latents.shape[0]
    print(f"generating {n} pil images from latents.")
    num_chunks = int(np.ceil(n / batch_size))
    chunks: Tuple[torch.Tensor] = torch.chunk(latents, num_chunks)
    
    pil_ims = []
    
    for latents in tqdm(chunks):
        latents = latents.to(device)  # (b, c, h, w)
        latents = VAE_DECODE_SCALE * latents
    
        with torch.autocast(device):
            ims = vae.decode(latents).sample
        
        ims = (ims / 2 + 0.5).clamp(0, 1)
        ims = ims.detach().cpu().permute(0, 2, 3, 1).numpy()
        ims = (ims * 255).round().astype("uint8")
        
        torch.cuda.empty_cache()
        pil_ims.extend([PIL.Image.fromarray(im) for im in ims])
    
    return pil_ims

# %% ../nbs/core.ipynb 33
def generate_image_grid(
    images: List[PIL.Image.Image], 
    nrow: int, 
    ncol: int):

    w, h = images[0].size # assume all images are of the same size
    grid = PIL.Image.new('RGB', size=(ncol * w, nrow * h))
    for i, im in enumerate(images): 
        grid.paste(im, box=(i % ncol * w, i // ncol * h))
    return grid

# %% ../nbs/core.ipynb 36
def get_image_size_from_aspect_ratio(aspect_ratio: float) -> Tuple[int, int]:
    base = 512
    width, height = (base, base)
    
    if aspect_ratio == 1:
        pass
    elif aspect_ratio < 1:
        height = base
        raw_width = round(height * aspect_ratio)
        multiplier = raw_width // 8
        width = 8 * multiplier
    elif aspect_ratio > 1:
        width = base
        raw_height = round(width / aspect_ratio)
        multiplier = raw_height // 8
        height = 8 * multiplier
    
    return (width, height)

# %% ../nbs/core.ipynb 39
def pil_to_b64(im: PIL.Image.Image, format="PNG") -> str:
    buffered = io.BytesIO()
    im.save(buffered, format=format)
    im_str = base64.b64encode(buffered.getvalue())
    return im_str.decode()

# %% ../nbs/core.ipynb 41
def b64_to_pil(b64: str, format="PNG") -> PIL.Image.Image:
    im = PIL.Image.open(io.BytesIO(base64.b64decode(b64)))
    try:
        im = im.convert(format)
    except ValueError:
        im = im.convert("RGB")
    
    return im

# %% ../nbs/core.ipynb 44
def file_to_b64(file_path: str) -> str:
    with open(file_path, "rb") as f:
        return base64.b64encode(f.read())

# %% ../nbs/core.ipynb 46
def b64_to_file(b64, output_path):
    with open(output_path, "wb") as f:
        f.write(base64.b64decode(b64))

# %% ../nbs/core.ipynb 48
def convert_gif_to_mp4(gif_path: str, crf=25, mp4_dir="mp4") -> str:
    mp4_dir = os.path.join(os.path.dirname(gif_path), mp4_dir)
    gif_file = gif_path.split(os.path.sep)[-1]
    
    if not os.path.exists(mp4_dir):
        os.makedirs(mp4_dir)
        
    mp4_path = os.path.join(mp4_dir, gif_file.replace(".gif", ".mp4"))
    cmd = """
    ffmpeg -y -i {} -movflags faststart -pix_fmt yuv420p -vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" 
    -crf {} -r 24 {}
    """
    os.system(cmd.format(gif_path, crf, mp4_path))
    return mp4_path

# %% ../nbs/core.ipynb 50
def latents_to_animation(
    latents: Union[torch.Tensor, List[torch.Tensor]],
    frame_idx_to_ms: dict = None,
    ms_per_frame: int = 300,
    animation_fname="latent_animation",
) -> str:
    """
    Generate an animation file from give `latents` and return the path of the animation file
    
    - latents: shape of (#latents, channel, height, width)
    """
    ims = latents_to_pils(latents)
    first_im, rest_ims = ims[0], ims[1:]
    n = len(ims)
    durations = [ms_per_frame] * n
    for frame_idx, ms in frame_idx_to_ms.items():
        durations[frame_idx] = ms
    
    # save as gif
    gif_fpath = os.path.join(tempfile.tempdir, f"{animation_fname}.gif")
    first_im.save(
        gif_fpath,
        format="gif",
        save_all=True,
        duration=durations,
        append_images=rest_ims,
    )
    
    # make infinite loop and optim
    # https://github.com/kohler/gifsicle
    os.system(f"""
    convert -loop 0 {gif_fpath} {gif_fpath} \
        && gifsicle -O3 --lossy=35 -o {gif_fpath} {gif_fpath}
    """)
    
    return gif_fpath

# %% ../nbs/core.ipynb 56
class Config:
    arbitrary_types_allowed = True

@pydantic.dataclasses.dataclass(config=Config)
class AIrtRequest:
    # inherit from `StableDiffusionPipeline`
    prompt: Union[str, List[str]]
    height: int = 512
    width: int = 512
    num_inference_steps: int = 30
    guidance_scale: float = 7.5
    negative_prompt: Union[List[str], str, None] = None
    num_images_per_prompt: Union[int, None] = 1
    eta: float = 0.0
        
    # inherif from `StableDiffusionImg2ImgPipeline`
    init_image: Union[torch.FloatTensor, PIL.Image.Image, str] = None # support b64
    strength: float = 0.6
    
    # custom parameters
    mode: str = None
    cmd: str = None
    animation_type: str = ""
    steps: int = 0
    cfg: float = None
    batch_size: int = None
    seed: int = None
    aspect_ratio: float = 1
    scheduler: str = "euler_a"
    
    # https://pydantic-docs.helpmanual.io/usage/validators/
    @pydantic.validator('steps')
    def steps_must_be_at_least_one(cls, v):
        if not v:
            return v
        
        if v < 1:
            raise ValueError("steps must be at least 1")
        return v
    
    
    @pydantic.validator('scheduler')
    def scheduler_must_be_available(cls, v):
        if v and not v in SCHEDULERS:
            raise ValueError(f"{v} is not a valid key in {SCHEDULERS.keys()}")
        return v.lower().strip()
    
    @pydantic.validator('animation_type')
    def animate_types_should_be_defined(cls, v):
        v = v.lower().strip().replace("-", "_")
        
        supported_animation_types =[
            "progress",
            "predict_x0"
        ]
        
        if v and not v in supported_animation_types:
            raise ValueError(f"{v} is not supported. Supported animation types: {supported_animation_types}")
        return v
    
    
    # https://pydantic-docs.helpmanual.io/usage/dataclasses/#initialize-hooks
    def __post_init_post_parse__(self, **kwargs):
        param_alias = {
            "steps": "num_inference_steps",
            "batch_size": "num_images_per_prompt",
            "cfg": "guidance_scale",
        }
        
        for custom_p, pipe_p in param_alias.items():
            custom_v = getattr(self, custom_p)
            pipe_v = getattr(self, pipe_p)
            
            if custom_v and custom_v != pipe_v:
                setattr(self, pipe_p, custom_v)
                
        # aspect ratio
        ar = self.aspect_ratio
        if ar != 1:
            width, height = get_image_size_from_aspect_ratio(ar)
            self.width = width
            self.height = height    
            
        # mode
        if self.init_image:
            self.mode = "image2image"
        else:
            self.mode = "text2image"
        
        # i2i image handling
        if isinstance(self.init_image, str):
            self.init_image = b64_to_pil(self.init_image)
            
        if isinstance(self.init_image, PIL.Image.Image):
            im = self.init_image
            w, h = im.size
            ar = round(w/h, 3)
            w, h = get_image_size_from_aspect_ratio(ar)
            
            self.width = w
            self.height = h
            self.aspect_ratio = ar
            self.init_image = im.resize((w, h))  # TODO: find the best resampling method
            

# %% ../nbs/core.ipynb 59
@pydantic.dataclasses.dataclass
class AIrtResponse:
    seed: int
    images: List[str] = None
    animation: str = None

        
    def keys(self) -> dict:
        return self.__dict__.keys()

# %% ../nbs/core.ipynb 61
def get_pipe_params_from_airt_req(req: AIrtRequest, pipe: StableDiffusionPipeline) -> dict:
    pipe_accepted_param_keys = inspect.signature(pipe).parameters.keys()
    pipe_params = {
        k: v for k, v in req.__dict__.items()
        if k in pipe_accepted_param_keys
    }
    return pipe_params

# %% ../nbs/core.ipynb 64
async def text2image(
    req: AIrtRequest, 
    return_pipe_out=False, 
    print_req=True
) -> Union[StableDiffusionPipelineOutput, AIrtResponse]:
    if print_req:
        pprint(req)
    
    generator = torch.Generator()
    seed = req.seed if req.seed else generator.seed()
    generator = torch.manual_seed(seed)
    
    scheduler_name = req.scheduler
    if scheduler_name:
        pipe.scheduler = SCHEDULERS[scheduler_name]
    
    pipe_params = get_pipe_params_from_airt_req(req, pipe)
    pipe_out = pipe(**pipe_params)
    torch.cuda.empty_cache()
    
    # set back to default scheduler
    pipe.scheduler = DEFAULT_SCHEDULER
        
    anim_b64 = None
    if req.animation_type:
        if req.animation_type == "progress":
            anim_path = latents_to_animation(
                pipe_out.all_latents,
                frame_idx_to_ms={-1: 2000},
                ms_per_frame=150,
            )
        elif req.animation_type == "predict-x0":
            anim_path = latents_to_animation(
                pipe_out.all_latents_x0,
                frame_idx_to_ms={-1: 2000},
                ms_per_frame=150,
            )
        else:
            raise NotImplementedError
            
        anim_b64 = file_to_b64(anim_path)

    ims_b64 = [pil_to_b64(im) for im in pipe_out.images]
    
    if return_pipe_out:
        return pipe_out
    else:
        return AIrtResponse(
            images=ims_b64,
            seed=seed,
            animation=anim_b64,
        )    

# %% ../nbs/core.ipynb 71
async def image2image(
    req: AIrtRequest, 
    return_pipe_out=False,
    print_req=True
) -> Union[StableDiffusionPipelineOutput, AIrtResponse]:
    if print_req:
        pprint(req)
        
    generator = torch.Generator()
    seed = req.seed if req.seed else generator.seed()
    generator = torch.manual_seed(seed)
    
    scheduler_name = req.scheduler
    if scheduler_name:
        pipe.scheduler = SCHEDULERS[scheduler_name]
    
    pipe_params = get_pipe_params_from_airt_req(req, i2i_pipe)
    pipe_out = i2i_pipe(**pipe_params)
    torch.cuda.empty_cache()
    
    # set back to default scheduler
    pipe.scheduler = DEFAULT_SCHEDULER
    
    anim_b64 = None
    if req.animation_type:
        if req.animation_type == "progress":
            anim_path = latents_to_animation(
                [pipe_out.init_scaled_latents] + pipe_out.all_latents,
                frame_idx_to_ms={0: 2000, 1: 2000, -1: 2000}
            )
        elif req.animation_type == "predict_x0":
            anim_path = latents_to_animation(
                [pipe_out.init_scaled_latents] + pipe_out.all_latents_x0,
                frame_idx_to_ms={0: 2000, 1: 2000, -1: 2000}
            )
        else:
            raise NotImplementedError
        
        anim_b64 = file_to_b64(anim_path)

    ims_b64 = [pil_to_b64(im) for im in pipe_out.images]
    
    if return_pipe_out:
        pipe_out.animation = anim_b64
        return pipe_out
    else:
        return AIrtResponse(
            images=ims_b64,
            seed=seed,
            animation=anim_b64,
        )    

# %% ../nbs/core.ipynb 84
async def handle_airt_request(req: AIrtRequest):
    pprint(req)
    mode = req.mode
    
    if mode == "text2image":
        return await text2image(req, print_req=False)
    elif mode == "image2image":
        return await image2image(req, print_req=False)
    else:
        raise NotImplementedError(req.mode)
